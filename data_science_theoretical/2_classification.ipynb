{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyODJgPEjQb5OdWdiIKn6uqn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vvtrip/ml_manifestations/blob/master/data_science_theoretical/2_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding classfiication\n"
      ],
      "metadata": {
        "id": "NZj_t4CUafhi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Assign categories to observations, and predict the category of a new observation (ex - classify mails as spam or legit)\n",
        "- Taking a set of predictor variables and a resposne variable as input\n",
        "- One of the Most widely used techniques for classification is Decision trees where we take a root node and make child nodes below it to claasify based on one of the two outcomes. \n",
        "- The root node and further child nodes are chosen based on lesser randomness\n",
        "- ex- If out of 6 outcomes, all are true then it is not random at all, while 3 true and 3 false is having total randomness.\n",
        " - in first case, P(true) = 1 and P(false) = 0\n",
        " - in second case, P(true) = 0.5 and P(false) = 0.5\n",
        "\n",
        "\n",
        "\n",
        "**ENTROPY**\n",
        "\n",
        " - Entropy is measure of degree of randomness\n",
        " and is measured as - (sum from k = 0 to n(Pi * log2(Pi))\n",
        " - Higher the entropy , higher the randomness\n",
        " - log base 2 is considered and negative sign is used\n",
        "\n",
        " -Entropy of a fair coin\n",
        "   - -(p(H)*log2P(H) + p(T) *log2P(T)) = 1\n",
        "            as P(H) = P(T) = 0.5\n",
        "\n",
        "\n",
        "-Entropy of an unfair coin\n",
        "   - -(p(H)*log2P(H) + p(T) *log2P(T)) = 0\n",
        "            if P(H) = 1 and P(T) = 0\n",
        "\n",
        "\n",
        "-Entropy of a partially fair coin\n",
        "   - -(p(H)*log2P(H) + p(T) *log2P(T)) = 0.81\n",
        "            if P(H) = 0.75 and P(T) = 0.25            \n"
      ],
      "metadata": {
        "id": "ymsj3mUNaizG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Information gain is the difference between previous entropy and next entropy(Applied after picking a root node)\n",
        "- The metric showing highest info gain is picked for root node and remaining metrics as its child nodes\n",
        "\n",
        "\n",
        "1.   Compute entropy of the target variable\n",
        "2.   Compute information gain by each predictor variable\n",
        "3.   Select the predictor variable with highest information gain\n",
        "4.   Further split the decision tree with the predictor variable and its values\n",
        "5.   Continue steps 2-5 until no significance information gain is possible\n",
        "6.   Return the descision tree\n",
        "\n",
        "\n",
        "\n",
        "OVERFIT AND UNDERFIT\n",
        "- The situation where model is performing too well on training data but the performance drops significantly on the test set is Overfitting.\n",
        "- creating very deep trees to fit a classification. Tree based algo are proone to overfitting\n",
        "- The situation where the model is performing poorly over the test and the train set is Underfitting\n",
        "\n",
        "|**True positive**| predicted yes | actual yes|   \n",
        "|**True negative**| predicted no | actual no  |   \n",
        "|**False positive**| predicted yes | actual no|   -- type 1 error   \n",
        "|**False negative**| predicted no | actual yes|   -- type 2 error\n",
        "\n",
        "\n",
        "|.................|**Predicted No| Predicted yes**|    \n",
        "|**Actual No**|......TN............|......FP...............|    \n",
        "|**Actual Yes**|.....FN............|......TP...............|\n",
        "\n",
        "**Measuring effectiveness**\n",
        "\n",
        "- Accuracy\n",
        "    - How often is the claasifies correct **(TP+TN)/ (TP+TN+FP+FN)**\n",
        "\n",
        "- Missclassification rate( error rate)\n",
        "    - How often is it wrong **(FP+FN)/ (TP+TN+FP+FN)**\n",
        "\n",
        "\n",
        "- True positivity rate( Sensitivity or Recall)\n",
        "    - How often does it predict yes **TP/ (TP+FN)**    \n",
        "\n",
        "\n",
        "\n",
        "- Precision\n",
        "    - When it predicts yes,how often is it correct **TP/ (TP+FP)**\n",
        "\n"
      ],
      "metadata": {
        "id": "3GaYYjpkfUKi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Some variations to classification"
      ],
      "metadata": {
        "id": "UFu3eJGyrVBw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Do not split if the size is too small\n",
        "- Do not split if the information gain is not enough\n",
        "- Do not split beyond a depth\n",
        "- Post pruning: remove leaf nodes that do not significantly increase the classfiication power\n",
        "\n",
        "\n",
        "**Measures of randomness**\n",
        " - Entropy\n",
        "    - -(sum from i=0 to n(Pi * log2(Pi))) \n",
        "    - has values between 0 and 1\n",
        "    - Complex match due to log calculation\n",
        "    - ID3 and C4.5 use Entrpy\n",
        "    - Entropy is considered good for attributes with classes\n",
        "    - Lower probabilities are not penalized\n",
        "\n",
        "\n",
        "- Gini Index\n",
        "    - 1 - (sum from i=0 to n(Pi^2))\n",
        "    - has values between 0 and 0.5\n",
        "    - Simpler math\n",
        "    - CART uses Gini\n",
        "    - Gini is considered fgood for continuous attributes\n",
        "    - Lower probabilities are penalized\n",
        "\n",
        "**Different Classsification technqiues**\n",
        "- Bagging Techniques\n",
        " - create trees using multiple subsets of data\n",
        " - Make classification by voting form the output of multiple tress\n",
        " - parallel\n",
        " - Random Forest\n",
        "\n",
        "\n",
        "- Boosting Techniques\n",
        " - create trees sequentially where each new tree is built to address the errors of the previous tree\n",
        " - Sequential\n",
        " - Gradient Descent\n",
        " - Adaptive Boost\n",
        " - XG BOoost\n",
        "\n",
        "- Other techniques \n",
        "  - Naive bayes classifier\n",
        "  - kNN\n",
        "  - Classification and regression trees\n",
        "  - Logistic regression\n",
        "\n"
      ],
      "metadata": {
        "id": "ESMI2o4trZ-q"
      }
    }
  ]
}