{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Distance Measure**\n",
        "\n",
        "Distance Measure is a very important aspect of clustering. Knowing how close or how far apart each variable is with respect to the other helps in grouping them.\n",
        "\n",
        "**Jaccard Distance**\n",
        "\n",
        "The Jaccard index is used to compare elements of two sets to identify which of the members are shared and not shared. The Jaccard Distance is a measure of how different the two given sets are.\n",
        "\n",
        "Jaccard Distance = 1-(Jaccard Index)\n",
        "\n",
        "\n",
        "**Euclidean Distance**\n",
        "\n",
        "Eucledian Distance is the shortest distance between the two given points in Eucledian Space.\n",
        "\n",
        "**Cosine Distance**\n",
        "\n",
        "Cosine distance of two given vectors u and v is the angular cosine between the given vectors.\n",
        "\n",
        "**Manhattan distance**\n",
        "\n",
        "Manhattan distance is calculated on a strictly horizontal or vertical path."
      ],
      "metadata": {
        "id": "QDou9qFF9fY4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**HIERARCHICAL CLUSTERING**\n",
        "\n",
        "- Begin by allotting each item to a cluster. If you are having N items, you are now having N clusters, where each of them contains one item. Now, let us make the similarities (distances) between the clusters the same as the similarities (distances) between the items they include.\n",
        "- Discover the most identical or closest pair of clusters, merge them into one cluster, thereby reducing one cluster.\n",
        "- Calculate the similarities (distances) between each of the old clusters and the new cluster.\n",
        "- Repeat step 2 and step 3 until all items are finally clustered into one cluster with size N."
      ],
      "metadata": {
        "id": "T0HeDI5l-CaF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DENDOGRAM**\n",
        "\n",
        "\n",
        "- A dendrogram is a branching diagram that represents the relationships of similarity among a group of entities\n",
        "- Each branch is called a clade\n",
        "- The terminal end of each clade is called a leaf\n",
        "- There is no limit to the number of leaves in a clade\n",
        "- The arrangement of the clades tells us which leaves are most similar to each other\n",
        "- The height of the branch points indicates how similar or different they are from each other\n",
        "- The greater the height, the greater the difference between the points"
      ],
      "metadata": {
        "id": "SYxG4MbF-aar"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Disadvantages of Agglomerative Clustering**\n",
        "\n",
        "Disadvantages for agglomerative hierarchical clustering\n",
        "\n",
        "- If data points are wrongly grouped at the inception, they cannot be reallocated.\n",
        "- If different similarity measures are utilized to calculate the similarity between clusters, it may result in different results altogether.\n",
        "\n",
        "**Tips for Hierarchical Clustering**\n",
        "\n",
        "There is no particular size that fits all solutions to determine how many clusters you need. It depends on what you intend to do with them. For a better solution, look at the basic characteristics of the given clusters at successive steps and make a decision when you have a solution that can be interpreted.\n",
        "\n",
        "**Hierarchical clustering â€“ Standardization**\n",
        "\n",
        "Standardizing the variables is a good way to follow while clustering data."
      ],
      "metadata": {
        "id": "MAegKXy0-n7_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**K-MEANS CLUSTERING**\n",
        "\n",
        "- Place k points in the space represented by the objects that are being clustered. These points represent initial group centroids.\n",
        "- Assign each object to the group that has the closest centroid.\n",
        "- When all objects have been assigned, recalculate the positions of the k centroids.\n",
        "- Repeat Step 2 and 3 until the centroids no longer move.\n",
        "\n",
        "Tips\n",
        "- For large datasets random sampling can be used to determine the k value for clustering\n",
        "- Hierarchical Clustering can also be used for the same\n",
        "\n",
        "**Choosing Right K-value**\n",
        "\n",
        "Other Ways to choose the right k value\n",
        "\n",
        "- Sampling\n",
        "- By rule of thumb\n",
        "- Elbow method\n",
        "- Information Criterion Approach\n",
        "- An Information Theoretic Approach\n",
        "- Choosing k using the Silhouette\n",
        "- Cross-validation"
      ],
      "metadata": {
        "id": "ym9lvwEN_D6f"
      }
    }
  ]
}