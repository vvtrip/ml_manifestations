{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vvtrip/ml_manifestations/blob/master/data_science_theoretical/3_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBqmfxwsvKls"
      },
      "source": [
        "## Understanding Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcKqMt5lvNsN"
      },
      "source": [
        "-  Determinig equations to capture elations between a variable and other varibles\n",
        "- Linear Regression creates linear mathematical relationship between two variables\n",
        "- y = mx + c\n",
        "  - y is dependent variable\n",
        "  - x is  independent variable\n",
        "  - m is slope, it captures how much y changes with a change in x\n",
        "  - captures steepness of the slope\n",
        "  - c is intercept, it captures where does the line intersect y axis\n",
        "  - captures values of y when x is zero and the effect of other variables on y\n",
        "\n",
        "- ex- fish_deaths = 9 *pollution_index + 112\n",
        "- with every increase in pollution index by 1 there are 9 more fish death during the year\n",
        "- even if water was perfectly pure there would be 112 death every year\n",
        "\n",
        "\n",
        "**How is it computed**\n",
        "- Approach 1: try brute-force: all combinaations of slope and intercept to find least squares solution\n",
        "- Approach 2: use an optimization algorithm\n",
        "- Approach 3: use an algebraic solution\n",
        "\n",
        "\n",
        "**Error**\n",
        "- OLS- Ordinary Least Square: (Actual - Predicted)^2\n",
        "- objective is to find values of slope and intercept that minimies the squared error\n",
        "\n",
        "\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFFF1xpOxvG9"
      },
      "source": [
        "# How is it computed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTJduE5oxymg"
      },
      "source": [
        "- Variance\n",
        "    - How is X spread around its mean\n",
        "    - Larger variance => Larger distance between numbers and the mean\n",
        "    - used by financial experts to measure an asset's volatility; A stock with higher variance comes with more risk and the potential for higher or lower returns\n",
        "    - (from 1 to n(Xi-Xmu)^2)/n, where Xmu is mean of X\n",
        "\n",
        "\n",
        "- CoVariance\n",
        "    - How X and Y vary from their mean values\n",
        "    - Can only gauge the direction of relationship (intandem or inverse)\n",
        "    - Covariance can also be used as a tool to diversify an inventor's portfolio; portfolio manager looks for investments that have a negative cobvariance to oe another\n",
        "    - (from 1 to n((Xi-Xmu) * (Yi-Ymu))/n, where Xmu is mean of X and where Ymu is mean of Y\n",
        "\n",
        "\n",
        "- Regression\n",
        "   - Slope: Change in y when x changes by 2 unit\n",
        "   - Slope = Covariance(x,y)/ variance(x)\n",
        "   - Regression Constant: Effect of other variables on y\n",
        "   - Regression Constant  = Mean(y) - Slope* Mean(X) \n",
        "\n",
        "Through trignometry also it can be verified\n",
        " - slope = (x2-x1)   /(y2-y2)\n",
        " - intercept= y2 - slope*x2\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eq2PCgW10rhQ"
      },
      "source": [
        "# Modelling\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnB812-t0uCP"
      },
      "source": [
        "**Training data**\n",
        "- basic subset of data on which to make model\n",
        "\n",
        "**Validation data**\n",
        "- Subset of data on which you test your model and do multiple back and forth for fine-tuning\n",
        "\n",
        "\n",
        "**Test data**\n",
        "- Subset of data that you use only once to assess the final performance\n",
        "\n",
        "can be split in 70/15/15 or 90/5/5\n",
        "\n",
        "\n",
        "\n",
        "**Residual sum of squares(RSS)**\n",
        "- measures rhe variation of modelling error\n",
        "- sum(actual - predicted)^2\n",
        "- in a way ordinary least square error is  model to estimate the regression line based on training data. While, RSS is a parameter to know the accuracy of model for both testing and training data.\n",
        "\n",
        "\n",
        "**Explained sum of squares(ESS)**\n",
        "- Explains how well ta regression model represents the modelled data\n",
        "- sum(predicted - average)^2\n",
        "\n",
        "**Total sum of squares(TSS)**\n",
        "- Quantifies total variation in sample\n",
        "- sum(actual - average)^2\n",
        "\n",
        "**coefficeint of determination(r^2 value)**\n",
        "- captires how much of the trend in the data set can be correctly predicted\n",
        "- ranges between 0 and 1(0 indicating worst fit)\n",
        "- 1 - (RSS/TSS)\n",
        "\n",
        "**average percent error**\n",
        "- catures percent variation from actual value\n",
        "- sum|(actual - predicted)|/|actual|) * 100\n",
        "\n",
        "**More error terms**\n",
        " - R square\n",
        " - Adjusted R^2\n",
        " - F Statistics\n",
        " - Mean squared error (MSE)\n",
        " - Mean absolute error (MAE)\n",
        " - Root mean square error (RMSE)\n",
        "\n",
        " \n",
        " **fix for overfitting**\n",
        "  - noise and random fluctuations also get picked up and learnt\n",
        "  - use resampling technqiue( k-fold cross validation)\n",
        "\n",
        "\n",
        " **fix for underfitting**\n",
        "  - cause: bad data or bad appraoch\n",
        "  - fix: choose better data or different approach\n",
        "\n",
        "\n",
        "  **Assumption and workarounds**\n",
        "\n",
        "\n",
        "  assumption: There exists a linear and additive relationship between dependent and independent variables\n",
        "\n",
        "  workaround: transform the variable using sqrt, log, square etc.\n",
        "\n",
        "\n",
        "  assumption: There must be no correlation among independent variabes(no multi-colinearity)\n",
        "\n",
        "  workaround: remove select correlated variables, Use penalized regression methods such as lasso, ridge etc\n",
        "\n",
        "  **Different regression techniques**\n",
        "  - Quantile - to get aggresive/conservative models\n",
        "  - Polynomial - to get non linear models\n",
        "  - Logistic - to deal with categorical dependent variables\n",
        "  - Lasso and Ridge - to prevent over/under fitting in case of too many features\n",
        "  - ElasticNet - to get the best of lasso and ridge\n",
        "  - Principal Component  - to deal with cases of too many features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSJL34fp0KcC"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyP6vWfGW8ZmtPGsk8JRou6C",
      "collapsed_sections": [
        "hBqmfxwsvKls",
        "QFFF1xpOxvG9",
        "eq2PCgW10rhQ"
      ],
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
